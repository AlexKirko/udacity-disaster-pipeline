{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amatamune\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amatamune\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\amatamune\\Anaconda3\\envs\\env_full\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import xgboost as xgb\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.corpus import stopwords as nl_stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "\n",
    "import dill\n",
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request    4464\n",
      "offer       118\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "engine = create_engine('sqlite:///database/disaster_response.db')\n",
    "df = pd.read_sql_table('categorized_messages', con=engine)\n",
    "Y = df.drop(['id','message','original','genre',\n",
    "            'related','request','offer','aid_related','direct_report',\n",
    "            ], axis=1)\n",
    "\n",
    "#Get service counts\n",
    "service_cnts = Y.sum(axis=0).sort_values(ascending=False)\n",
    "requests_and_offers = df[['request','offer']].sum(axis=0)\n",
    "top10 = service_cnts.iloc[:10]\n",
    "print(requests_and_offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7286, 3441, 2917, 2452, 2440, 2308, 2149, 2081, 1705, 1669],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing missing dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from database\n",
    "engine = create_engine('sqlite:///database/disaster_response.db')\n",
    "df = pd.read_sql_table('categorized_messages', con=engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into explanotary variable (tweet text) \n",
    "# and the target (message classes)\n",
    "# We also save the genre, although I ended up not using it.\n",
    "\n",
    "X_text = df['message']\n",
    "X_genre = df['genre']\n",
    "Y = df.drop(['id','message','original','genre'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the data if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weather update - a cold front from Cuba that c...\n",
       "1              Is the Hurricane over or is it not over\n",
       "2                      Looking for someone but no name\n",
       "3    UN reports Leogane 80-90 destroyed. Only Hospi...\n",
       "4    says: west side of Haiti, rest of the country ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        0      0            0             0                 0   \n",
       "3        1        1      0            1             0                 1   \n",
       "4        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
       "0                  0         0         0            0  ...            0   \n",
       "1                  0         0         0            0  ...            0   \n",
       "2                  0         0         0            0  ...            0   \n",
       "3                  0         0         0            0  ...            0   \n",
       "4                  0         0         0            0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "3     0              0              0  \n",
       "4     0              0              0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text,stopwords=None):\n",
    "    \"\"\"\n",
    "    Function performs basic tokenization:\n",
    "    1. Conversion to lowercase\n",
    "    2. Removal of special characters\n",
    "    3. Tokenization using NLTK\n",
    "    4. Removal of stopwords\n",
    "    \n",
    "    Args:\n",
    "    text (str): text to be tokenized\n",
    "    \n",
    "    Out:\n",
    "    words (list): a list of tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9]',\" \",text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if stopwords:\n",
    "        words = [w for w in words if w not in stopwords]\n",
    "    \n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pass', 'haiti']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that it works\n",
    "stopwords = nl_stopwords.words('english')\n",
    "tokenize(X_text.iloc[0], stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related                   20252\n",
       "request                    4464\n",
       "offer                       118\n",
       "aid_related               10841\n",
       "medical_help               2081\n",
       "medical_products           1311\n",
       "search_and_rescue           724\n",
       "security                    471\n",
       "military                    859\n",
       "child_alone                   0\n",
       "water                      1669\n",
       "food                       2917\n",
       "shelter                    2308\n",
       "clothing                    404\n",
       "money                       603\n",
       "missing_people              298\n",
       "refugees                    874\n",
       "death                      1192\n",
       "other_aid                  3441\n",
       "infrastructure_related     1705\n",
       "transport                  1199\n",
       "buildings                  1331\n",
       "electricity                 532\n",
       "tools                       159\n",
       "hospitals                   283\n",
       "shops                       120\n",
       "aid_centers                 309\n",
       "other_infrastructure       1151\n",
       "weather_related            7286\n",
       "floods                     2149\n",
       "storm                      2440\n",
       "fire                        282\n",
       "earthquake                 2452\n",
       "cold                        528\n",
       "other_weather              1376\n",
       "direct_report              5064\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The child-alone category is 0 for all observations. \n",
    "# We should remove it and then just predict it to be 0 independent of text.\n",
    "\n",
    "#Check for redundant variables in the class list - this is how we find child-alone\n",
    "Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.drop(['child_alone'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26175</td>\n",
       "      <td>26175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26176</td>\n",
       "      <td>26176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26177</td>\n",
       "      <td>26177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26178</td>\n",
       "      <td>26178</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26179</td>\n",
       "      <td>26179</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26180 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  related  request  offer  aid_related  medical_help  \\\n",
       "0          0        1        0      0            0             0   \n",
       "1          1        1        0      0            1             0   \n",
       "2          2        1        0      0            0             0   \n",
       "3          3        1        1      0            1             0   \n",
       "4          4        1        0      0            0             0   \n",
       "...      ...      ...      ...    ...          ...           ...   \n",
       "26175  26175        0        0      0            0             0   \n",
       "26176  26176        0        0      0            0             0   \n",
       "26177  26177        1        0      0            0             0   \n",
       "26178  26178        1        0      0            1             0   \n",
       "26179  26179        1        0      0            0             0   \n",
       "\n",
       "       medical_products  search_and_rescue  security  military  ...  \\\n",
       "0                     0                  0         0         0  ...   \n",
       "1                     0                  0         0         0  ...   \n",
       "2                     0                  0         0         0  ...   \n",
       "3                     1                  0         0         0  ...   \n",
       "4                     0                  0         0         0  ...   \n",
       "...                 ...                ...       ...       ...  ...   \n",
       "26175                 0                  0         0         0  ...   \n",
       "26176                 0                  0         0         0  ...   \n",
       "26177                 0                  0         0         0  ...   \n",
       "26178                 0                  0         0         1  ...   \n",
       "26179                 0                  0         0         0  ...   \n",
       "\n",
       "       aid_centers  other_infrastructure  weather_related  floods  storm  \\\n",
       "0                0                     0                0       0      0   \n",
       "1                0                     0                1       0      1   \n",
       "2                0                     0                0       0      0   \n",
       "3                0                     0                0       0      0   \n",
       "4                0                     0                0       0      0   \n",
       "...            ...                   ...              ...     ...    ...   \n",
       "26175            0                     0                0       0      0   \n",
       "26176            0                     0                0       0      0   \n",
       "26177            0                     0                0       0      0   \n",
       "26178            0                     0                0       0      0   \n",
       "26179            0                     0                0       0      0   \n",
       "\n",
       "       fire  earthquake  cold  other_weather  direct_report  \n",
       "0         0           0     0              0              0  \n",
       "1         0           0     0              0              0  \n",
       "2         0           0     0              0              0  \n",
       "3         0           0     0              0              0  \n",
       "4         0           0     0              0              0  \n",
       "...     ...         ...   ...            ...            ...  \n",
       "26175     0           0     0              0              0  \n",
       "26176     0           0     0              0              0  \n",
       "26177     0           0     0              0              0  \n",
       "26178     0           0     0              0              0  \n",
       "26179     0           0     0              0              0  \n",
       "\n",
       "[26180 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resetting indeces is necessary to avoid\n",
    "# ValueError: WRITEBACKIFCOPY base is read-only\n",
    "X_text.reset_index()\n",
    "Y.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form the sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_text, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20944,)\n",
      "(20944, 35)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using XGBoost as the backbone of the MultiOutputClassifier\n",
    "# It shows better results taht Random Forest and scikit-learn basic gradient boosting\n",
    "pipeline_basic = Pipeline([\n",
    "    ('vec',CountVectorizer(tokenizer=lambda x: tokenize(x, stopwords))),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    \n",
    "    #('clf',MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42)))\n",
    "    #('clf',MultiOutputClassifier(estimator=GradientBoostingClassifier(random_state=42)))\n",
    "    ('clf',MultiOutputClassifier(estimator=xgb.XGBClassifier(random_state=42,n_estimators=100,subsample=1)))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...le_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_basic.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=42, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_basic.steps[2][1].estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipeline_basic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "related\n",
      "F1-score is 0.7049406548565185\n",
      "1\n",
      "request\n",
      "F1-score is 0.5802382620882972\n",
      "2\n",
      "offer\n",
      "F1-score is 0.0\n",
      "3\n",
      "aid_related\n",
      "F1-score is 0.6330275229357798\n",
      "4\n",
      "medical_help\n",
      "F1-score is 0.27016885553470915\n",
      "5\n",
      "medical_products\n",
      "F1-score is 0.4089635854341736\n",
      "6\n",
      "search_and_rescue\n",
      "F1-score is 0.16666666666666669\n",
      "7\n",
      "security\n",
      "F1-score is 0.018867924528301886\n",
      "8\n",
      "military\n",
      "F1-score is 0.2880658436213992\n",
      "9\n",
      "water\n",
      "F1-score is 0.717041800643087\n",
      "10\n",
      "food\n",
      "F1-score is 0.7845884413309983\n",
      "11\n",
      "shelter\n",
      "F1-score is 0.6219839142091154\n",
      "12\n",
      "clothing\n",
      "F1-score is 0.5161290322580645\n",
      "13\n",
      "money\n",
      "F1-score is 0.35802469135802467\n",
      "14\n",
      "missing_people\n",
      "F1-score is 0.31746031746031744\n",
      "15\n",
      "refugees\n",
      "F1-score is 0.2834008097165992\n",
      "16\n",
      "death\n",
      "F1-score is 0.5351351351351351\n",
      "17\n",
      "other_aid\n",
      "F1-score is 0.12403100775193798\n",
      "18\n",
      "infrastructure_related\n",
      "F1-score is 0.07124681933842239\n",
      "19\n",
      "transport\n",
      "F1-score is 0.3257328990228013\n",
      "20\n",
      "buildings\n",
      "F1-score is 0.3958333333333334\n",
      "21\n",
      "electricity\n",
      "F1-score is 0.3333333333333333\n",
      "22\n",
      "tools\n",
      "F1-score is 0.0\n",
      "23\n",
      "hospitals\n",
      "F1-score is 0.030769230769230767\n",
      "24\n",
      "shops\n",
      "F1-score is 0.0\n",
      "25\n",
      "aid_centers\n",
      "F1-score is 0.10126582278481013\n",
      "26\n",
      "other_infrastructure\n",
      "F1-score is 0.03125000000000001\n",
      "27\n",
      "weather_related\n",
      "F1-score is 0.7093980077955826\n",
      "28\n",
      "floods\n",
      "F1-score is 0.6591999999999999\n",
      "29\n",
      "storm\n",
      "F1-score is 0.6485819975339089\n",
      "30\n",
      "fire\n",
      "F1-score is 0.4\n",
      "31\n",
      "earthquake\n",
      "F1-score is 0.8282138794084187\n",
      "32\n",
      "cold\n",
      "F1-score is 0.49696969696969695\n",
      "33\n",
      "other_weather\n",
      "F1-score is 0.14545454545454548\n",
      "34\n",
      "direct_report\n",
      "F1-score is 0.4860557768924304\n"
     ]
    }
   ],
   "source": [
    "# Note: classification report isn't very helpful\n",
    "# we'll just output the f1-score and use that to judge model quality.\n",
    "# If we had more time to put into this project,\n",
    "# AUC would also be a great candidate to base model selection on\n",
    "for ind, col in enumerate(list(Y_test.columns)):\n",
    "    #print(ind)\n",
    "    y_test = list(Y_test.iloc[:,ind])\n",
    "    y_pred = list(Y_pred[:,ind])\n",
    "    #print(y_test)\n",
    "    #print(y_pred)\n",
    "    #break\n",
    "    print(col)\n",
    "    try:\n",
    "        print('F1-score is {}'.format(f1_score(y_test, y_pred)))\n",
    "    except:\n",
    "        print('F1-score is {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    #print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a simple parameter grid. Even this will require training 6 models\n",
    "parameters = {\n",
    "    'clf__estimator__n_estimators': [100, 200],\n",
    "    'clf__estimator__subsample': [0.8, 1],\n",
    "    'clf__estimator__max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# See what we wind up with\n",
    "cv_basic = GridSearchCV(pipeline_basic, param_grid=parameters, cv=3, n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...le_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "           n_jobs=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=6,\n",
       "       param_grid={'clf__estimator__n_estimators': [100, 200], 'clf__estimator__subsample': [0.8, 1], 'clf__estimator__max_depth': [2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_basic.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_..._pos_weight=1, seed=None, silent=None,\n",
       "       subsample=0.8, verbosity=1),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the model choice we settled on\n",
    "cv_basic.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=4, min_child_weight=1, missing=None,\n",
       "       n_estimators=200, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=42, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the estimator parameters we settled on\n",
    "cv_basic.best_estimator_.steps[2][1].estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the results\n",
    "Y_cv = cv_basic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "request\n",
      "F1-score is 0.6345646437994723\n",
      "offer\n",
      "F1-score is 0.0\n",
      "aid_related\n",
      "F1-score is 0.6767151767151767\n",
      "medical_help\n",
      "F1-score is 0.3395585738539898\n",
      "medical_products\n",
      "F1-score is 0.4690721649484536\n",
      "search_and_rescue\n",
      "F1-score is 0.23958333333333334\n",
      "security\n",
      "F1-score is 0.0900900900900901\n",
      "military\n",
      "F1-score is 0.4229390681003584\n",
      "water\n",
      "F1-score is 0.7320872274143303\n",
      "food\n",
      "F1-score is 0.7829937998228522\n",
      "shelter\n",
      "F1-score is 0.6674937965260546\n",
      "clothing\n",
      "F1-score is 0.5079365079365079\n",
      "money\n",
      "F1-score is 0.42458100558659223\n",
      "missing_people\n",
      "F1-score is 0.3636363636363636\n",
      "refugees\n",
      "F1-score is 0.3308270676691729\n",
      "death\n",
      "F1-score is 0.5824742268041238\n",
      "other_aid\n",
      "F1-score is 0.2028639618138425\n",
      "infrastructure_related\n",
      "F1-score is 0.14285714285714285\n",
      "transport\n",
      "F1-score is 0.36923076923076925\n",
      "buildings\n",
      "F1-score is 0.4666666666666667\n",
      "electricity\n",
      "F1-score is 0.40789473684210525\n",
      "tools\n",
      "F1-score is 0.06896551724137931\n",
      "hospitals\n",
      "F1-score is 0.14084507042253522\n",
      "shops\n",
      "F1-score is 0.0\n",
      "aid_centers\n",
      "F1-score is 0.11904761904761905\n",
      "other_infrastructure\n",
      "F1-score is 0.0534351145038168\n",
      "weather_related\n",
      "F1-score is 0.7550266721378744\n",
      "floods\n",
      "F1-score is 0.6697819314641744\n",
      "storm\n",
      "F1-score is 0.6689895470383276\n",
      "fire\n",
      "F1-score is 0.3953488372093023\n",
      "earthquake\n",
      "F1-score is 0.8391451068616423\n",
      "cold\n",
      "F1-score is 0.5268817204301076\n",
      "other_weather\n",
      "F1-score is 0.22875816993464054\n",
      "direct_report\n",
      "F1-score is 0.5340838023764853\n"
     ]
    }
   ],
   "source": [
    "# Again we focus on the F1-scores. Classification report\n",
    "# is uncommented only for submission as it isn't very useful.\n",
    "cv_basic_fscores = {}\n",
    "for ind, col in enumerate(list(Y_test.columns)):\n",
    "    y_test = list(Y_test.iloc[:,ind])\n",
    "    y_cv = list(Y_cv[:,ind])\n",
    "    #print(y_test)\n",
    "    #print(y_pred)\n",
    "    #break\n",
    "    print(col)\n",
    "    try:\n",
    "        print('F1-score is {}'.format(f1_score(y_test, y_cv)))\n",
    "        cv_basic_fscores[col] = f1_score(y_test, y_cv)\n",
    "    except:\n",
    "        pass\n",
    "    #print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more regularization\n",
    "The first things we can do is add column subsampling for different trees. This tends to make sure that the model gets all it can from the less significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced pipeline\n",
    "# add colsample_bytree\n",
    "pipeline_advanced = Pipeline([\n",
    "    ('vec',CountVectorizer(tokenizer=lambda x: tokenize(x, stopwords))),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf',MultiOutputClassifier(estimator=xgb.XGBClassifier(\n",
    "        random_state=42,n_estimators=200,subsample=0.8,max_depth=4,learning_rate=0.1,colsample_bytree=0.4)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_..._pos_weight=1, seed=None,\n",
       "       silent=None, subsample=0.8, verbosity=1),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_advanced.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "F1-score is 0.751148734275228\n",
      "request\n",
      "F1-score is 0.6221928665785997\n",
      "offer\n",
      "F1-score is 0.0\n",
      "aid_related\n",
      "F1-score is 0.6758656599843791\n",
      "medical_help\n",
      "F1-score is 0.34529914529914535\n",
      "medical_products\n",
      "F1-score is 0.4578947368421052\n",
      "search_and_rescue\n",
      "F1-score is 0.20105820105820105\n",
      "security\n",
      "F1-score is 0.07207207207207207\n",
      "military\n",
      "F1-score is 0.38686131386861317\n",
      "water\n",
      "F1-score is 0.725\n",
      "food\n",
      "F1-score is 0.7915194346289752\n",
      "shelter\n",
      "F1-score is 0.6741293532338307\n",
      "clothing\n",
      "F1-score is 0.53125\n",
      "money\n",
      "F1-score is 0.4285714285714286\n",
      "missing_people\n",
      "F1-score is 0.2903225806451613\n",
      "refugees\n",
      "F1-score is 0.32452830188679244\n",
      "death\n",
      "F1-score is 0.5744125326370757\n",
      "other_aid\n",
      "F1-score is 0.18465227817745805\n",
      "infrastructure_related\n",
      "F1-score is 0.1108433734939759\n",
      "transport\n",
      "F1-score is 0.3682539682539683\n",
      "buildings\n",
      "F1-score is 0.47596153846153855\n",
      "electricity\n",
      "F1-score is 0.42105263157894735\n",
      "tools\n",
      "F1-score is 0.0\n",
      "hospitals\n",
      "F1-score is 0.028985507246376815\n",
      "shops\n",
      "F1-score is 0.0\n",
      "aid_centers\n",
      "F1-score is 0.12048192771084337\n",
      "other_infrastructure\n",
      "F1-score is 0.07547169811320754\n",
      "weather_related\n",
      "F1-score is 0.7477253928866832\n",
      "floods\n",
      "F1-score is 0.6603773584905661\n",
      "storm\n",
      "F1-score is 0.6744457409568262\n",
      "fire\n",
      "F1-score is 0.35\n",
      "earthquake\n",
      "F1-score is 0.8426966292134831\n",
      "cold\n",
      "F1-score is 0.5444444444444444\n",
      "other_weather\n",
      "F1-score is 0.20666666666666667\n",
      "direct_report\n",
      "F1-score is 0.5329153605015674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Check results\n",
    "Y_pred = pipeline_advanced.predict(X_test)\n",
    "advanced_fscores = {}\n",
    "for ind, col in enumerate(list(Y_test.columns)):\n",
    "    y_test = list(Y_test.iloc[:,ind])\n",
    "    y_pred = list(Y_pred[:,ind])\n",
    "    y_score = list(Y_score[ind][:,1])\n",
    "    #print(y_test)\n",
    "    #print(y_pred)\n",
    "    #break\n",
    "    print(col)\n",
    "    try:\n",
    "        advanced_fscores[col] = f1_score(y_test, y_pred)\n",
    "    except:\n",
    "        advanced_fscores[col] = f1_score(y_test, y_pred, average='weighted')\n",
    "    print('F1-score is {}'.format(advanced2_fscores[col]))    \n",
    "       \n",
    "    #print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The F1-scores are better pretty much across the board. From now on, we'll continue to use column subsampling. But there is more to be done if we want to improve F1-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of balance\n",
    "Our classes are unbalanced across the board. The ratio is different everywhere but it's, in general, something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19631\n",
       "1     1313\n",
       "Name: water, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train['water'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need is to give more weight to the positives. Currently, our model has excellent precision and bad recall. But we can give up a bit of precision (have responders discard some messages that do get to them) than it is to miss messages in the noise.\n",
    "I decided to assign the positive observations weight=3. Frankly, this parameter should be properly tuned, but this is outside the scope of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced pipeline\n",
    "# add colsample_bytree\n",
    "pipeline_advanced2 = Pipeline([\n",
    "    ('vec',CountVectorizer(tokenizer=lambda x: tokenize(x, stopwords))),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf',MultiOutputClassifier(estimator=xgb.XGBClassifier(\n",
    "        random_state=42,n_estimators=200,subsample=0.8,max_depth=4,\n",
    "        learning_rate=0.1,colsample_bytree=0.4,scale_pos_weight=3)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_..._pos_weight=3, seed=None,\n",
       "       silent=None, subsample=0.8, verbosity=1),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_advanced2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "F1-score is 0.751148734275228\n",
      "request\n",
      "F1-score is 0.6720430107526881\n",
      "offer\n",
      "F1-score is 0.0625\n",
      "aid_related\n",
      "F1-score is 0.7156549520766773\n",
      "medical_help\n",
      "F1-score is 0.4718792866941015\n",
      "medical_products\n",
      "F1-score is 0.52\n",
      "search_and_rescue\n",
      "F1-score is 0.3113207547169811\n",
      "security\n",
      "F1-score is 0.1\n",
      "military\n",
      "F1-score is 0.5031847133757962\n",
      "water\n",
      "F1-score is 0.7714285714285714\n",
      "food\n",
      "F1-score is 0.79232693911593\n",
      "shelter\n",
      "F1-score is 0.7085590465872155\n",
      "clothing\n",
      "F1-score is 0.5401459854014599\n",
      "money\n",
      "F1-score is 0.4642857142857142\n",
      "missing_people\n",
      "F1-score is 0.3768115942028985\n",
      "refugees\n",
      "F1-score is 0.419672131147541\n",
      "death\n",
      "F1-score is 0.641255605381166\n",
      "other_aid\n",
      "F1-score is 0.4251357641582622\n",
      "infrastructure_related\n",
      "F1-score is 0.22633744855967078\n",
      "transport\n",
      "F1-score is 0.42896935933147634\n",
      "buildings\n",
      "F1-score is 0.5526838966202783\n",
      "electricity\n",
      "F1-score is 0.5196078431372549\n",
      "tools\n",
      "F1-score is 0.06451612903225806\n",
      "hospitals\n",
      "F1-score is 0.1951219512195122\n",
      "shops\n",
      "F1-score is 0.05405405405405405\n",
      "aid_centers\n",
      "F1-score is 0.20618556701030927\n",
      "other_infrastructure\n",
      "F1-score is 0.1386138613861386\n",
      "weather_related\n",
      "F1-score is 0.7856879380302473\n",
      "floods\n",
      "F1-score is 0.692528735632184\n",
      "storm\n",
      "F1-score is 0.7422885572139304\n",
      "fire\n",
      "F1-score is 0.4536082474226804\n",
      "earthquake\n",
      "F1-score is 0.8419889502762431\n",
      "cold\n",
      "F1-score is 0.5471698113207546\n",
      "other_weather\n",
      "F1-score is 0.3786407766990291\n",
      "direct_report\n",
      "F1-score is 0.588235294117647\n"
     ]
    }
   ],
   "source": [
    "#Check results\n",
    "Y_pred = pipeline_advanced2.predict(X_test)\n",
    "advanced2_fscores = {}\n",
    "for ind, col in enumerate(list(Y_test.columns)):\n",
    "    y_test = list(Y_test.iloc[:,ind])\n",
    "    y_pred = list(Y_pred[:,ind])\n",
    "    #print(y_test)\n",
    "    #print(y_pred)\n",
    "    #break\n",
    "    print(col)\n",
    "    try:\n",
    "        advanced2_fscores[col] = f1_score(y_test, y_pred)\n",
    "    except:\n",
    "        advanced2_fscores[col] = f1_score(y_test, y_pred, average='weighted')\n",
    "    print('F1-score is {}'.format(advanced2_fscores[col]))    \n",
    "       \n",
    "    #print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have improved across most classes. There is one last improvement I'd like to make before moving on from this project.\n",
    "Hope you find it fun to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec incorporation - outline\n",
    "My strategy was to try to move from pure bag-of-words to something that also takes into account the meaning of words and sentence length/punctuation.\n",
    "To measure emotional distress I calculate sentence length statistics for tweets and also the number of various punctuation marks.\n",
    "To associate meaning with tokens, I use GoogleNews pretrained Word2Vec model. Then I clusterized all the words in the training corpus using KMeans. I then calculate three most prominent clusters within each tweet (if two clusters have the same number of members in the tweet, then the cluster that has lower global frequency is given priority)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning to the reviewer\n",
    "If you are reviewing this work, then I recommend that you don't run the following section, as it requires you to download the `GoogleNews-vectors-negative300.bin`, which is larger than 3GB. I'm quite proud of the idea and that it worked, but reading the source code should suffice.\n",
    "If you'd like to test the model, just skip down and load the pickled model, see `pipeline_advanced4 = ...`\n",
    "You can then test it.\n",
    "If you do want to download Google's word2Vec, then it's available [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in word2vec\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./.word2vec/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The goal is to create a mapping that maps a word to a word cluster\n",
    "#We'll later use the three most prominent cluster numbers as factors\n",
    "#and try to improve the model this way\n",
    "\n",
    "#First we need to get all the words into one array\n",
    "all_words = []\n",
    "i=0\n",
    "for sent in X_text:\n",
    "    all_words += tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we map words to vectors\n",
    "unique_words = set(all_words)\n",
    "unique_words.update('supercollider')\n",
    "unique_words = list(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allow words to not be found\n",
    "def get_vector_func(w2v, w_placeholder='supercollider'):\n",
    "    \"\"\"\n",
    "    The function sets the Word2Vec model\n",
    "    for the inner get_vector function\n",
    "    and returns it.\n",
    "    \n",
    "    Args:\n",
    "    w2v (Word2VecKeyedVectors) - a Word2Vec model\n",
    "    w_placeholder (str) - the word that we'll replace\n",
    "        missing words with. Doesn't matter what it is\n",
    "        as long as it's rare and has nothing to do\n",
    "        with natural disasters\n",
    "    \n",
    "    Out:\n",
    "    try_get_vector (func) - a function that allows\n",
    "    words tobe missing from the vocabulary\n",
    "    \"\"\"\n",
    "    def try_get_vector(word):\n",
    "        \"\"\"\n",
    "        This inner function implements exception handling\n",
    "        for Word2VecKeyedVectors.get_vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vect = w2v.get_vector(word)\n",
    "        except:\n",
    "            # Doesn't matter what we use for words that aren't found\n",
    "            # as long as it's rare and has nothing to do with\n",
    "            # natural disasters\n",
    "            vect = w2v.get_vector(w_placeholder)\n",
    "        return vect\n",
    "    return try_get_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_get_vector = get_vector_func(w2v_model)\n",
    "words_mappings = {word:vect for word,vect in zip(unique_words, map(try_get_vector,unique_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We begin by building a couple feature engineering transformers\n",
    "class SentLenExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class extracts average sentence length and standard deviation\n",
    "    of the sentence length from a document.\n",
    "    I don't expect this to improve the model. This transformer and\n",
    "    the next were exercises leading to w2vClusters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        sent_lengths (list): sentence lengths\n",
    "        mess_col (str): name of the message column\n",
    "        \"\"\"\n",
    "        self.sent_lengths = None\n",
    "    \n",
    "    def calc_sent_lengths(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        if sentence_list:\n",
    "            sent_lengths = [len(s) for s in sentence_list]\n",
    "        else:\n",
    "            sent_lengths = [0]\n",
    "        return sent_lengths\n",
    "    \n",
    "    def len_mean(self, text):\n",
    "        if self.sent_lengths is None:\n",
    "            self.calc_sent_lengths(text)\n",
    "        return np.mean(self.sent_lengths)\n",
    "    \n",
    "    def len_std(self, text):\n",
    "        if self.sent_lengths is None:\n",
    "            self.calc_sent_lengths(text)\n",
    "        return np.std(self.sent_lengths)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        SL = X.apply(self.calc_sent_lengths)\n",
    "        X_mean = SL.apply(np.mean).rename('sent_len_mean')\n",
    "        X_std = SL.apply(np.std).rename('sent_len_std')\n",
    "        X_len = pd.concat([X_mean, X_std], axis=1)\n",
    "        return X_len\n",
    "\n",
    "class PunktCounter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class calculates the number of punctuation characters in text\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_quest = X.apply(lambda x: x.count('?')).rename('quest_cnt')\n",
    "        X_comma = X.apply(lambda x: x.count(',')).rename('comma_cnt')\n",
    "        X_exclam = X.apply(lambda x: x.count('!')).rename('excl_cnt')\n",
    "        X_punct = pd.concat([X_quest, X_comma, X_exclam], axis=1)\n",
    "        return X_punct\n",
    "\n",
    "# TO ADD: feature engineering: word2vec and clusterization\n",
    "class w2vClusters(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_clusters, words_mappings, \n",
    "                 random_state=42, n_jobs=1, tokenize=tokenize):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        n_clusters (int) - number of clusters to use for KMeans\n",
    "        words_mappings (dict) - a dict mapping words to vectors\n",
    "        random_state (float) - initialization random state for KMeans\n",
    "        n_jobs (int) - number of parallel jobs for KMeans\n",
    "        tokenize (func) - sentence tokenizing function\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.words_mappings = words_mappings\n",
    "        self.tokenize = tokenize\n",
    "        self.cl_model = KMeans(n_clusters=n_clusters, random_state=random_state, n_jobs=n_jobs)\n",
    "    \n",
    "    def ws_to_vs(self, words):\n",
    "        def w_to_v(word):\n",
    "            try:\n",
    "                vec = self.words_mappings[word]\n",
    "            except:\n",
    "                # This is meant as a harmless joke\n",
    "                # I very well know the dangers of hard-coding\n",
    "                # stuff like this deep into the implementation\n",
    "                vec = self.words_mappings['supercollider']\n",
    "            return vec\n",
    "        vecs = list(map(w_to_v,words))\n",
    "        return vecs\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the kmeans model that is used to assign clusters\n",
    "        to words\n",
    "        \"\"\"\n",
    "        X_token = X.apply(self.tokenize)\n",
    "        words = list(itertools.chain.from_iterable(X_token))\n",
    "        vecs = self.ws_to_vs(words)\n",
    "        self.cl_model.fit(vecs)        \n",
    "\n",
    "        # Tweets are short, so we need a conflict resolution mechanism for\n",
    "        # when we'll have just one word in a second or third most\n",
    "        # frequent cluster\n",
    "        #\n",
    "        # We'll be prioritizing the less frequent clusters, so we need to\n",
    "        # calculate the frequencies\n",
    "        clusters = self.cl_model.predict(vecs)\n",
    "        self.cl_counts = pd.Series(clusters).value_counts()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Attributes words to KMeans clusters and outputs three\n",
    "        clusters with the highest frequencies in a tweet. In\n",
    "        case of conflicts takes the globally less frequent tweets.\n",
    "\n",
    "        Args:\n",
    "        X - tweet series\n",
    "        \"\"\"\n",
    "        # Clean and extract words\n",
    "        def count_clusters(tweet):\n",
    "            words = self.tokenize(tweet)\n",
    "            \n",
    "            if not words:\n",
    "                return [-1000000, -1000000, -1000000]\n",
    "            # Check if we got any words\n",
    "            vecs = self.ws_to_vs(words)\n",
    "        \n",
    "            # Get clusters\n",
    "            clusters = self.cl_model.predict(vecs)\n",
    "\n",
    "            # Count words in each cluster and sort\n",
    "            x_cl_counts = pd.Series(clusters).value_counts().sort_values(ascending=False)\n",
    "            #Get three most prominent clusters\n",
    "            prev_count = 0\n",
    "            cls = []\n",
    "            curr_idces = []\n",
    "            for index, item in x_cl_counts.iteritems():\n",
    "                if item < prev_count:\n",
    "                    # Among the most frequent local clusters pick\n",
    "                    # the least frequent global clusters\n",
    "                    gl_cl_counts = self.cl_counts.loc[curr_idces].sort_values(ascending=True)\n",
    "                    to_add = min(3-len(cls),len(gl_cl_counts))\n",
    "                    cls += list(gl_cl_counts.iloc[:to_add].index)\n",
    "                    if len(cls) >= 3:\n",
    "                        break\n",
    "                    curr_idces = []\n",
    "                curr_idces.append(index)\n",
    "                prev_count = item\n",
    "\n",
    "            # If we didn't get three clusters, add the final ones\n",
    "            if len(cls) < 3:\n",
    "                gl_cl_counts = self.cl_counts.loc[curr_idces].sort_values(ascending=True)\n",
    "                to_add = min(3-len(cls),len(gl_cl_counts))\n",
    "                cls += list(gl_cl_counts.iloc[:to_add].index)\n",
    "\n",
    "            # If still not enough, pad with -1000000\n",
    "            if len(cls) < 3:\n",
    "                cls += [-1000000] * (3 - len(cls))\n",
    "            return cls\n",
    "        X_clusters = X.apply(count_clusters)\n",
    "        # From https://stackoverflow.com/questions/35491274/pandas-split-column-of-lists-into-multiple-columns\n",
    "        X_clusters = pd.DataFrame(X_clusters.values.tolist(), index=X_clusters.index, \n",
    "                                  columns=['cluster1','cluster2','cluster3'])\n",
    "        return X_clusters\n",
    "    \n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        print(X)\n",
    "        # what other output you want\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sent_len_mean    24.5\n",
       "sent_len_std     12.5\n",
       "Name: 4291, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing transformers\n",
    "pc = SentLenExtractor()\n",
    "pc.fit(X_train.iloc[:1000])\n",
    "res = pc.transform(X_train[:1000])\n",
    "res.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Word2Vec transformer\n",
    "w2v_cl = w2vClusters(n_clusters=30, n_jobs=3, words_mappings=words_mappings, tokenize=tokenize)\n",
    "w2v_cl.fit(X_train.iloc[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = w2v_cl.transform(X_train[:1000])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tkn(x):\n",
    "    return tokenize(x, stopwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pipeline incorporates the w2vClusters transformer to incorporate some\n",
    "# word meanings into the model\n",
    "s_len = Pipeline([\n",
    "    ('sl_extract',SentLenExtractor()),\n",
    "    ('sl_scale',MinMaxScaler())    \n",
    "])\n",
    "\n",
    "s_punct = Pipeline([\n",
    "    ('sp_extract',PunktCounter()),\n",
    "    ('sp_scale',MinMaxScaler())   \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Advanced pipeline\n",
    "pipeline_advanced3 = Pipeline([\n",
    "    #('debug1',Debug()),\n",
    "    ('feat', FeatureUnion(\n",
    "        [('BoW',Pipeline(\n",
    "            [('vec',CountVectorizer(tokenizer=tkn)),\n",
    "            ('tfidf',TfidfTransformer())])),\n",
    "        #('sentlen',s_len),\n",
    "        #('punkt',s_punct),\n",
    "        ('cl_freqs',Pipeline([\n",
    "            # I tried different n_clusters here. Between 20 and 30 works best\n",
    "            # Too little, and everything ends up in one cluster\n",
    "            # Too much, and every word in a tweet is in a different cluster.\n",
    "            ('w2v',w2vClusters(n_clusters=27, n_jobs=3, words_mappings=words_mappings, tokenize=tokenize)),\n",
    "            ('one_hot',OneHotEncoder(categories='auto',handle_unknown='ignore'))]))\n",
    "        ])),\n",
    "    #('debug2',Debug()),\n",
    "    ('clf',MultiOutputClassifier(estimator=xgb.XGBClassifier(\n",
    "        random_state=42,n_estimators=200,subsample=0.8,max_depth=4,\n",
    "        learning_rate=0.1,colsample_bytree=0.4,scale_pos_weight=3)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('feat',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('BoW',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('vec',\n",
       "                                                                  CountVectorizer(analyzer='word',\n",
       "                                                                                  binary=False,\n",
       "                                                                                  decode_error='strict',\n",
       "                                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                                  encoding='utf-8',\n",
       "                                                                                  input='content',\n",
       "                                                                                  lowercase=True,\n",
       "                                                                                  max_df=1.0,\n",
       "                                                                                  max_features=None,\n",
       "                                                                                  min_df=1,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               1),\n",
       "                                                                                  preprocessor=None,\n",
       "                                                                                  stop_words=No...\n",
       "                                                               colsample_bylevel=1,\n",
       "                                                               colsample_bynode=1,\n",
       "                                                               colsample_bytree=0.4,\n",
       "                                                               gamma=0,\n",
       "                                                               learning_rate=0.1,\n",
       "                                                               max_delta_step=0,\n",
       "                                                               max_depth=4,\n",
       "                                                               min_child_weight=1,\n",
       "                                                               missing=None,\n",
       "                                                               n_estimators=200,\n",
       "                                                               n_jobs=1,\n",
       "                                                               nthread=None,\n",
       "                                                               objective='binary:logistic',\n",
       "                                                               random_state=42,\n",
       "                                                               reg_alpha=0,\n",
       "                                                               reg_lambda=1,\n",
       "                                                               scale_pos_weight=3,\n",
       "                                                               seed=None,\n",
       "                                                               silent=None,\n",
       "                                                               subsample=0.8,\n",
       "                                                               verbosity=1),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_advanced3.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "F1-score is 0.7599912211808767\n",
      "request\n",
      "F1-score is 0.6801530891197376\n",
      "offer\n",
      "F1-score is 0.0625\n",
      "aid_related\n",
      "F1-score is 0.7193378480060195\n",
      "medical_help\n",
      "F1-score is 0.47282608695652173\n",
      "medical_products\n",
      "F1-score is 0.5178571428571428\n",
      "search_and_rescue\n",
      "F1-score is 0.30985915492957744\n",
      "security\n",
      "F1-score is 0.11199999999999999\n",
      "military\n",
      "F1-score is 0.5\n",
      "water\n",
      "F1-score is 0.7554980595084086\n",
      "food\n",
      "F1-score is 0.7906588824020016\n",
      "shelter\n",
      "F1-score is 0.7035830618892508\n",
      "clothing\n",
      "F1-score is 0.5217391304347827\n",
      "money\n",
      "F1-score is 0.4711111111111111\n",
      "missing_people\n",
      "F1-score is 0.4057971014492754\n",
      "refugees\n",
      "F1-score is 0.4158415841584159\n",
      "death\n",
      "F1-score is 0.6486486486486487\n",
      "other_aid\n",
      "F1-score is 0.4208885424785659\n",
      "infrastructure_related\n",
      "F1-score is 0.2505050505050505\n",
      "transport\n",
      "F1-score is 0.40443213296398894\n",
      "buildings\n",
      "F1-score is 0.5386138613861386\n",
      "electricity\n",
      "F1-score is 0.5148514851485149\n",
      "tools\n",
      "F1-score is 0.06451612903225806\n",
      "hospitals\n",
      "F1-score is 0.20930232558139533\n",
      "shops\n",
      "F1-score is 0.05128205128205128\n",
      "aid_centers\n",
      "F1-score is 0.2150537634408602\n",
      "other_infrastructure\n",
      "F1-score is 0.12121212121212122\n",
      "weather_related\n",
      "F1-score is 0.7920289855072463\n",
      "floods\n",
      "F1-score is 0.7011494252873564\n",
      "storm\n",
      "F1-score is 0.7363184079601991\n",
      "fire\n",
      "F1-score is 0.48421052631578954\n",
      "earthquake\n",
      "F1-score is 0.8438538205980066\n",
      "cold\n",
      "F1-score is 0.5436893203883496\n",
      "other_weather\n",
      "F1-score is 0.35910224438902744\n",
      "direct_report\n",
      "F1-score is 0.6065420560747664\n"
     ]
    }
   ],
   "source": [
    "# Testing the efficiency\n",
    "Y_pred = pipeline_advanced3.predict(X_test)\n",
    "advanced3_fscores = {}\n",
    "\n",
    "for ind, col in enumerate(list(Y_test.columns)):\n",
    "    y_test = list(Y_test.iloc[:,ind])\n",
    "    y_pred = list(Y_pred[:,ind])\n",
    "    #print(y_test)\n",
    "    #print(y_pred)\n",
    "    #break\n",
    "    print(col)\n",
    "    try:\n",
    "        advanced3_fscores[col] = f1_score(y_test, y_pred)\n",
    "    except:\n",
    "        advanced3_fscores[col] = f1_score(y_test, y_pred, average='weighted')\n",
    "    print('F1-score is {}'.format(advanced3_fscores[col]))  \n",
    "    \n",
    "    #print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.008842486905648705\n",
      "request\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.008110078367049467\n",
      "offer\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.0\n",
      "aid_related\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.0036828959293422336\n",
      "medical_help\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.0009468002624202221\n",
      "medical_products\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.002142857142857224\n",
      "search_and_rescue\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.001461599787403678\n",
      "security\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.011999999999999983\n",
      "military\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.0031847133757961776\n",
      "water\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.015930511920162704\n",
      "food\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.0016680567139284452\n",
      "shelter\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.004975984697964675\n",
      "clothing\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.018406854966677155\n",
      "money\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.006825396825396901\n",
      "missing_people\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.028985507246376885\n",
      "refugees\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.0038305469891251054\n",
      "death\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.007393043267482691\n",
      "other_aid\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.0042472216796963225\n",
      "infrastructure_related\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.02416760194537973\n",
      "transport\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.024537226367487397\n",
      "buildings\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.014070035234139633\n",
      "electricity\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.004756357988740079\n",
      "tools\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.0\n",
      "hospitals\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.01418037436188313\n",
      "shops\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.00277200277200277\n",
      "aid_centers\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.008868196430550923\n",
      "other_infrastructure\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.017401740174017383\n",
      "weather_related\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.00634104747699904\n",
      "floods\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.008620689655172376\n",
      "storm\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.005970149253731294\n",
      "fire\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.030602278893109125\n",
      "earthquake\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.0018648703217635232\n",
      "cold\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.0034804909324049893\n",
      "other_weather\n",
      "F1-score with word2Vec - F1-score with column subsample: -0.01953853231000169\n",
      "direct_report\n",
      "F1-score with word2Vec - F1-score with column subsample: 0.018306761957119444\n",
      "Cumulative F1-score difference between word2Vec-including model and the next-best model is 0.04136314753955766\n"
     ]
    }
   ],
   "source": [
    "cumsum = 0\n",
    "for col, item in advanced2_fscores.items():\n",
    "    print(col)\n",
    "    delta = advanced3_fscores[col]-item\n",
    "    print(\"F1-score with word2Vec - F1-score with column subsample: {}\".format(delta))\n",
    "    cumsum += delta\n",
    "print(\"Cumulative F1-score difference between word2Vec-including model and the next-best model is {}\".format(cumsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on further work\n",
    "There is a lot more than can be done. I got the word2Vec-including model to the point where it has a positive impact (and doesn't react too strongly to a change in the number of clusters), but there is room to choose different clusterization algorithms, and engineer different features from them.\n",
    "There is room for good old-fashioned hyper-parameter optimization. We could have started with widely different classifiers and then polled them using log-reg or done something similar.\n",
    "Testing these ideas is outside the scope of this project, but maybe I'll return to them when I work on my final project, if I pick a natural language processing problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline_advanced3.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline_advanced3,'pipeline_advanced3.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_advanced4 = joblib.load('pipeline_advanced3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_advanced4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.2'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'water', 'food', 'shelter', 'clothing', 'money', 'missing_people',\n",
       "       'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport',\n",
       "       'buildings', 'electricity', 'tools', 'hospitals', 'shops',\n",
       "       'aid_centers', 'other_infrastructure', 'weather_related', 'floods',\n",
       "       'storm', 'fire', 'earthquake', 'cold', 'other_weather',\n",
       "       'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = Y_test.columns\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import xgboost as xgb\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.corpus import stopwords as nl_stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "#I've rearranged the functions a bit to tailor them to my implementation\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    \"\"\"\n",
    "    Load data and split it into target and explanotary variables\n",
    "    \n",
    "    Args:\n",
    "    database_filepath(str): path to the database\n",
    "    \n",
    "    Out:\n",
    "    X_text(Series): a Series of messages\n",
    "    \"\"\"\n",
    "    engine = create_engine('sqlite:///database/disaster_response.db')\n",
    "    df = pd.read_sql_table('categorized_messages', con=engine)\n",
    "    X_text = df['message']\n",
    "    X_genre = df['genre']\n",
    "    Y = df.drop(['id','message','original','genre'], axis=1)\n",
    "    Y = Y.drop(['child_alone'], axis=1)\n",
    "    X_text.reset_index()\n",
    "    Y.reset_index()\n",
    "    return X_text, Y\n",
    "\n",
    "def tokenize(text,stopwords=None):\n",
    "    \"\"\"\n",
    "    Function performs basic tokenization:\n",
    "    1. Conversion to lowercase\n",
    "    2. Removal of special characters\n",
    "    3. Tokenization using NLTK\n",
    "    4. Removal of stopwords\n",
    "    \n",
    "    Args:\n",
    "    text (str): text to be tokenized\n",
    "    \n",
    "    Out:\n",
    "    words (list): a list of tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9]',\" \",text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if stopwords:\n",
    "        words = [w for w in words if w not in stopwords]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def get_vector_func(w2v, w_placeholder='supercollider'):\n",
    "    \"\"\"\n",
    "    The function sets the Word2Vec model\n",
    "    for the inner get_vector function\n",
    "    and returns it.\n",
    "\n",
    "    Args:\n",
    "    w2v (Word2VecKeyedVectors) - a Word2Vec model\n",
    "    w_placeholder (str) - the word that we'll replace\n",
    "        missing words with. Doesn't matter what it is\n",
    "        as long as it's rare and has nothing to do\n",
    "        with natural disasters\n",
    "\n",
    "    Out:\n",
    "    try_get_vector (func) - a function that allows\n",
    "    words tobe missing from the vocabulary\n",
    "    \"\"\"\n",
    "    def try_get_vector(word):\n",
    "        \"\"\"\n",
    "        This inner function implements exception handling\n",
    "        for Word2VecKeyedVectors.get_vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vect = w2v.get_vector(word)\n",
    "        except:\n",
    "            # Doesn't matter what we use for words that aren't found\n",
    "            # as long as it's rare and has nothing to do with\n",
    "            # natural disasters\n",
    "            vect = w2v.get_vector(w_placeholder)\n",
    "        return vect\n",
    "    return try_get_vector\n",
    "\n",
    "#We begin by building a couple feature engineering transformers\n",
    "class SentLenExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class extracts average sentence length and standard deviation\n",
    "    of the sentence length from a document.\n",
    "    I don't expect this to improve the model. This transformer and\n",
    "    the next were exercises leading to w2vClusters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        sent_lengths (list): sentence lengths\n",
    "        mess_col (str): name of the message column\n",
    "        \"\"\"\n",
    "        self.sent_lengths = None\n",
    "    \n",
    "    def calc_sent_lengths(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        if sentence_list:\n",
    "            sent_lengths = [len(s) for s in sentence_list]\n",
    "        else:\n",
    "            sent_lengths = [0]\n",
    "        return sent_lengths\n",
    "    \n",
    "    def len_mean(self, text):\n",
    "        if self.sent_lengths is None:\n",
    "            self.calc_sent_lengths(text)\n",
    "        return np.mean(self.sent_lengths)\n",
    "    \n",
    "    def len_std(self, text):\n",
    "        if self.sent_lengths is None:\n",
    "            self.calc_sent_lengths(text)\n",
    "        return np.std(self.sent_lengths)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        SL = X.apply(self.calc_sent_lengths)\n",
    "        X_mean = SL.apply(np.mean).rename('sent_len_mean')\n",
    "        X_std = SL.apply(np.std).rename('sent_len_std')\n",
    "        X_len = pd.concat([X_mean, X_std], axis=1)\n",
    "        return X_len\n",
    "\n",
    "class PunktCounter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class calculates the number of punctuation characters in text\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_quest = X.apply(lambda x: x.count('?')).rename('quest_cnt')\n",
    "        X_comma = X.apply(lambda x: x.count(',')).rename('comma_cnt')\n",
    "        X_exclam = X.apply(lambda x: x.count('!')).rename('excl_cnt')\n",
    "        X_punct = pd.concat([X_quest, X_comma, X_exclam], axis=1)\n",
    "        return X_punct\n",
    "\n",
    "# TO ADD: feature engineering: word2vec and clusterization\n",
    "class w2vClusters(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_clusters, words_mappings, \n",
    "                 random_state=42, n_jobs=1, tokenize=tokenize):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        n_clusters (int) - number of clusters to use for KMeans\n",
    "        words_mappings (dict) - a dict mapping words to vectors\n",
    "        random_state (float) - initialization random state for KMeans\n",
    "        n_jobs (int) - number of parallel jobs for KMeans\n",
    "        tokenize (func) - sentence tokenizing function\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.words_mappings = words_mappings\n",
    "        self.tokenize = tokenize\n",
    "        self.cl_model = KMeans(n_clusters=n_clusters, random_state=random_state, n_jobs=n_jobs)\n",
    "    \n",
    "    def ws_to_vs(self, words):\n",
    "        def w_to_v(word):\n",
    "            try:\n",
    "                vec = self.words_mappings[word]\n",
    "            except:\n",
    "                # This is meant as a harmless joke\n",
    "                # I very well know the dangers of hard-coding\n",
    "                # stuff like this deep into the implementation\n",
    "                vec = self.words_mappings['supercollider']\n",
    "            return vec\n",
    "        vecs = list(map(w_to_v,words))\n",
    "        return vecs\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the kmeans model that is used to assign clusters\n",
    "        to words\n",
    "        \"\"\"\n",
    "        X_token = X.apply(self.tokenize)\n",
    "        words = list(itertools.chain.from_iterable(X_token))\n",
    "        vecs = self.ws_to_vs(words)\n",
    "        self.cl_model.fit(vecs)        \n",
    "\n",
    "        # Tweets are short, so we need a conflict resolution mechanism for\n",
    "        # when we'll have just one word in a second or third most\n",
    "        # frequent cluster\n",
    "        #\n",
    "        # We'll be prioritizing the less frequent clusters, so we need to\n",
    "        # calculate the frequencies\n",
    "        clusters = self.cl_model.predict(vecs)\n",
    "        self.cl_counts = pd.Series(clusters).value_counts()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Attributes words to KMeans clusters and outputs three\n",
    "        clusters with the highest frequencies in a tweet. In\n",
    "        case of conflicts takes the globally less frequent tweets.\n",
    "\n",
    "        Args:\n",
    "        X - tweet series\n",
    "        \"\"\"\n",
    "        # Clean and extract words\n",
    "        def count_clusters(tweet):\n",
    "            words = self.tokenize(tweet)\n",
    "            \n",
    "            if not words:\n",
    "                return [-1000000, -1000000, -1000000]\n",
    "            # Check if we got any words\n",
    "            vecs = self.ws_to_vs(words)\n",
    "        \n",
    "            # Get clusters\n",
    "            clusters = self.cl_model.predict(vecs)\n",
    "\n",
    "            # Count words in each cluster and sort\n",
    "            x_cl_counts = pd.Series(clusters).value_counts().sort_values(ascending=False)\n",
    "            #Get three most prominent clusters\n",
    "            prev_count = 0\n",
    "            cls = []\n",
    "            curr_idces = []\n",
    "            for index, item in x_cl_counts.iteritems():\n",
    "                if item < prev_count:\n",
    "                    # Among the most frequent local clusters pick\n",
    "                    # the least frequent global clusters\n",
    "                    gl_cl_counts = self.cl_counts.loc[curr_idces].sort_values(ascending=True)\n",
    "                    to_add = min(3-len(cls),len(gl_cl_counts))\n",
    "                    cls += list(gl_cl_counts.iloc[:to_add].index)\n",
    "                    if len(cls) >= 3:\n",
    "                        break\n",
    "                    curr_idces = []\n",
    "                curr_idces.append(index)\n",
    "                prev_count = item\n",
    "\n",
    "            # If we didn't get three clusters, add the final ones\n",
    "            if len(cls) < 3:\n",
    "                gl_cl_counts = self.cl_counts.loc[curr_idces].sort_values(ascending=True)\n",
    "                to_add = min(3-len(cls),len(gl_cl_counts))\n",
    "                cls += list(gl_cl_counts.iloc[:to_add].index)\n",
    "\n",
    "            # If still not enough, pad with -1000000\n",
    "            if len(cls) < 3:\n",
    "                cls += [-1000000] * (3 - len(cls))\n",
    "            return cls\n",
    "        X_clusters = X.apply(count_clusters)\n",
    "        # From https://stackoverflow.com/questions/35491274/pandas-split-column-of-lists-into-multiple-columns\n",
    "        X_clusters = pd.DataFrame(X_clusters.values.tolist(), index=X_clusters.index, \n",
    "                                  columns=['cluster1','cluster2','cluster3'])\n",
    "        return X_clusters\n",
    "    \n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        print(X)\n",
    "        # what other output you want\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "def build_model(words_mappings,tokenize,stopwords):\n",
    "    # This pipeline incorporates the w2vClusters transformer to incorporate some\n",
    "    # word meanings into the model\n",
    "    s_len = Pipeline([\n",
    "        ('sl_extract',SentLenExtractor()),\n",
    "        ('sl_scale',MinMaxScaler())    \n",
    "    ])\n",
    "\n",
    "    s_punct = Pipeline([\n",
    "        ('sp_extract',PunktCounter()),\n",
    "        ('sp_scale',MinMaxScaler())   \n",
    "\n",
    "    ])\n",
    "\n",
    "    # Advanced pipeline\n",
    "    pipeline_advanced3 = Pipeline([\n",
    "        #('debug1',Debug()),\n",
    "        ('feat', FeatureUnion(\n",
    "            [('BoW',Pipeline(\n",
    "                [('vec',CountVectorizer(tokenizer=lambda x: tokenize(x, stopwords))),\n",
    "                ('tfidf',TfidfTransformer())])),\n",
    "            #('sentlen',s_len),\n",
    "            #('punkt',s_punct),\n",
    "            ('cl_freqs',Pipeline([\n",
    "                # I tried different n_clusters here. Between 20 and 30 works best\n",
    "                # Too little, and everything ends up in one cluster\n",
    "                # Too much, and every word in a tweet is in a different cluster.\n",
    "                ('w2v',w2vClusters(n_clusters=27, n_jobs=3, words_mappings=words_mappings, tokenize=tokenize)),\n",
    "                ('one_hot',OneHotEncoder(categories='auto',handle_unknown='ignore'))]))\n",
    "            ])),\n",
    "        #('debug2',Debug()),\n",
    "        ('clf',MultiOutputClassifier(estimator=xgb.XGBClassifier(\n",
    "            random_state=42,n_estimators=200,subsample=0.8,max_depth=4,\n",
    "            learning_rate=0.1,colsample_bytree=0.4,scale_pos_weight=3)))\n",
    "    ])\n",
    "    pipeline_advanced3.fit()\n",
    "    \n",
    "    return pipeline_advanced3\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    # Testing the efficiency\n",
    "    Y_pred = model.predict(X_test)\n",
    "    model_fscores = {}\n",
    "\n",
    "    for ind, col in enumerate(list(Y_test.columns)):\n",
    "        y_test = list(Y_test.iloc[:,ind])\n",
    "        y_pred = list(Y_pred[:,ind])\n",
    "        print(col)\n",
    "        try:\n",
    "            model_fscores[col] = f1_score(y_test, y_pred)\n",
    "        except:\n",
    "            model_fscores[col] = f1_score(y_test, y_pred, average='weighted')\n",
    "        print('F1-score is {}'.format(advanced3_fscores[col]))  \n",
    "\n",
    "        #print(classification_report(y_test,y_pred))\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    joblib.dump(model,model_filepath)\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) == 3:\n",
    "        database_filepath, model_filepath = sys.argv[1:]\n",
    "        print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        X_text, Y = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_text, Y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        stopwords = nl_stopwords.words('english')\n",
    "        #Make preparations for the word2Vec part\n",
    "        try:\n",
    "            #Load in word2vec\n",
    "            w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./.word2vec/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "        except:\n",
    "            raise ValueError('Word2Vec missing at ./.word2vec/GoogleNews-vectors-negative300.bin')\n",
    "        \n",
    "        #The goal is to create a mapping that maps a word to a word cluster\n",
    "        #We'll later use the three most prominent cluster numbers as factors\n",
    "        #and try to improve the model this way\n",
    "\n",
    "        #First we need to get all the words into one array\n",
    "        all_words = []\n",
    "        i=0\n",
    "        for sent in X_text:\n",
    "            all_words += tokenize(sent)\n",
    "            \n",
    "        #Now we map words to vectors\n",
    "        unique_words = set(all_words)\n",
    "        unique_words.update('supercollider')\n",
    "        unique_words = list(unique_words)\n",
    "        \n",
    "        #Allow words to not be found\n",
    "        try_get_vector = get_vector_func(w2v_model)\n",
    "        words_mappings = {word:vect for word,vect in zip(unique_words, map(try_get_vector,unique_words))}\n",
    "        \n",
    "        print('Building model...')\n",
    "        model = build_model(words_mappings,tokenize,stopwords)\n",
    "        \n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        print('Evaluating model...')\n",
    "        evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "    else:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument and the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument. \\n\\nExample: python '\\\n",
    "              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
